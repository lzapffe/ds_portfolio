---
title: "Web Scraping Stock Data"
---

My beloved dad has disowned my many times due to my unfiltered comments and said that the money  will now instead go to starting a non-profit supporting orphaned owls. So, I need to get back on his good side. In addition, he spends a few days every year gathering last years' data on the stocks that he is following. As a programmer, I know that everything that is tedious and done manually, can be done automatically with programming. So, this project will try to automate my dad's yearly data collection. The goals for this project is:

- To get un-disowned from my dad
- To save my dad some days every year collecting his data
- Learning to use and get comfortable with web scraping
- Learning more about web scraping as a tool to gather data
- Get experience with using web scraping and problem solving for some less straightforward scenarios of gathering data
- To use programming to solve a real-world problem


## The data to be gathered

The current script will work with a list of 10 random stocks and try to get the data from those.


```{r loading-libraries}
# To read in the list of stocks from an Excel file
library(readxl)
# To check path access for web scraping
library(robotstxt)
# Get web scraping and html related functions
library(rvest)
```


## Loading the data into a data frame

Reading the xlsx file into r:
```{r read xlsx}
data_df <- read_xlsx("Liste over fondsnavn utvalg.xlsx")
```


## Checking access to URL

We need to get access to the following URL: https://morningstar.no/no/. So, I will check whether we can web scrape it here:
```{r path-access}
paths_allowed("https://morningstar.no/no/")
```
So far, so good. We have access to the webpage.

## The plan - workflow

Getting the desired data, involves searching up each stock on their webpage, retrieving the URL for the webpage of that stock, adding a tag to the URL to get to the history tab for that stock, then extract the relevant data for the desired year.

We have to search up each URL, since the tag they use to index each stock seems to be a random number, and is unfortunately not based on the stock's ID.


## Get the URLs

```{r get-URL-search}
# Here is the URL for searching for the first page: https://www.morningstar.no/no/screener/fund.aspx#?filtersSelectedValue=%7B%22term%22:%22LU0048574536%22%7D&page=1&sortField=legalName&sortOrder=asc

search_page <- read_html("https://www.morningstar.no/no/screener/fund.aspx#?filtersSelectedValue=%7B%22term%22:%22LU0048574536%22%7D&page=1&sortField=legalName&sortOrder=asc")

```

Get the URL of the actual stock page:
```{r get-URL-page}
stock_URL <- search_page %>%
  html_elements(".ec-table__investment-link")
  #html_elements("link")
stock_URL

#sess <- read_html_live("https://www.morningstar.no/no/screener/fund.aspx#?filtersSelectedValue=%7B%22term%22:%22LU0048574536%22%7D&page=1&sortField=legalName&sortOrder=asc")
#sess$view()
```

It looks like we might be able to get rvest to click on stuff on the webpage by using html$click in rvest or clickElement() in RSelenium



## Web scraping Basics

Taken from https://r4ds.hadley.nz/webscraping

Every html page has a html element that consists of two children: a header and a body. 

href contains links

You can use read_html() from rvest to get a html object

CSS = Cascading style sheets. It is a tool for defining visual styling of html. You can use CSS selectors to scrape data as they define patterns for locating html elements. For example, .title, where title is a specific title string, will grab the matching element with the class "title".

html_elements() will give you all elements matches, while html_element() will give you as many outputs as you gave inputs. If there are more outputs, than inputs, the function will return the first outputs. If you are searching for an element that doesn't exist, html_element() will return a missing value and html_elements() will return a list of 0. You typically use html_elements() to get the relevant strings/observations and html_element() to get the one variable you need from the string. If you use html_element() to get the variable and there isn't one, it will just put NA in its place. However, if you use html_elements() if will give you one less row/observation, which can make it hard to match the vector with another vector or data frame, since the lenghts will be different.

html_text2() - extracts the plain text from a html string or object. It removes all tags, attributes, and so on.
html_attr() - extracts data from attributes. For example the URL from href.
html_table() - extracts a html table

If a page has a table, you can use html_element("table") and pipe it into html_table(). Then you can use select() to get the variables/columns you need.


### Finding selectors

One of the hardest parts of webscraping is finding the selectors you need to get the data you want. You can use the tool SelectorGadget in Chrome. You can also right click on an element and chose inspect to get more information about it. Chrome is the best for this. Inside the elements view, you can right-click on an element and chose "copy as selector" to get a selector that will extract just that data.

[This webpage - called Selectors Explained](https://kittygiraudel.github.io/selectors-explained/) provides plain English explanations of CSS selectors that you can provide it.

[This webpage - Select the plates](https://flukeout.github.io/) has a game/tutorial you can work your way through to learn more about selectors.

[This webpage](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_selectors) has throurough information on different CSS selectors


## Dynamic sites

If what you see on the webpage isn't the same as in the html object you get, you are probably dealing with a dynamic page. In that case, html_element() and rvest won't work, since they download the static html object. Instead, the webpage dynamically makes the content with javascript. To webscrape these dynamic pages, you need to simulate running the webpage, so that Javascript is running, while extracting data.


## To do list

- Figure out how to read dynamic html webpages
- Look into their API and get the data from there too
- Get the Python data set and get data from there too
- Discuss the benefits and drawbacks of the 3 different approaches for getting the data from the webpage